[{"authors":["admin"],"categories":null,"content":"Bin Wang is currently a full-time researcher of Beijing Institute for General Artificial Intelligence, BIGAI. Before joining BIGAI, she was a researcher of Advanced Innovation Center for Future Visual Entertainment in Beijing Film Academy from 2017 to 2021. She earned her Ph.D in November 2012 from Beihang University, advised by Prof. Shuling Dai. From Aug. 2010 to Aug. 2012, she experienced two-year\u0026rsquo;s wonderful life in the Sensorimotor System Lab at University of British Columbia, Canada, working with Prof. Dinesh K. Pai and Prof. François Faure, from University Joseph Fourier, Grenoble, France. She has been a Research Fellow from April 2013 to April 2014 in National University of Singapore, supervised by Prof. Kangkang Yin. From 2014 to 2016, she did her postdoctoral researcher at Visual Computing Research Center, leading by Prof. Hui Huang.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://BinWangBFA.github.io/author/bin-wang-%E7%8E%8B-%E6%BB%A8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bin-wang-%E7%8E%8B-%E6%BB%A8/","section":"authors","summary":"Bin Wang is currently a full-time researcher of Beijing Institute for General Artificial Intelligence, BIGAI. Before joining BIGAI, she was a researcher of Advanced Innovation Center for Future Visual Entertainment","tags":null,"title":"Bin Wang (王 滨)","type":"authors"},{"authors":["YUCHEN SUN^  —— CFCS, Peking University","XINGYU NI^ —— CFCS, Peking University","BO ZHU —— Dartmouth College","BIN WANG* —— AICFVE, Beijing Film Academy","BAOQUAN CHEN* —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","(^ equal contribution, * corresponding author)"],"categories":[],"content":"","date":1621094229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621094229,"objectID":"744560083ada1e1bcd95750b1d761292","permalink":"https://BinWangBFA.github.io/publication/siga21_mpm/","publishdate":"2021-05-15T23:57:09+08:00","relpermalink":"/publication/siga21_mpm/","section":"publication","summary":"We propose a novel numerical scheme to simulate interactions between a magnetic field and nonlinearly magnetized objects immersed in it.","tags":["Physical simulation","Physics"],"title":"A Material Point Method for Nonlinearly Magnetized Materials","type":"publication"},{"authors":["LIANGWANG RUAN^ —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","JINYUAN LIU^ —— Dartmouth College","BO ZHU —— Dartmouth College","SHINJIRO SUEDA —— Texas A\u0026M","BIN WANG* —— AICFVE, Beijing Film Academy","BAOQUAN CHEN* —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","(^ equal contribution, * corresponding author)"],"categories":[],"content":"","date":1621094229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621094229,"objectID":"24483fb85718a4479a679d540d41bc4f","permalink":"https://BinWangBFA.github.io/publication/sig21_waterstrider/","publishdate":"2021-05-15T23:57:09+08:00","relpermalink":"/publication/sig21_waterstrider/","section":"publication","summary":"This paper proposes a novel three-way coupling framework to simulate the surface-tension-dominant contact between rigid and fluid, which using a Lagrangian surface membrane to handle the interactions between bodies and fluid.","tags":["Physical simulation","Physics"],"title":"Solid-Fluid Interaction with Surface-Tension-Dominant Contact","type":"publication"},{"authors":["QINGZHE GAO —— Shandong University, Beijing Film Academy","BIN WANG —— AICFVE, Beijing Film Academy","LIBIN LIU —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","BAOQUAN CHEN —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy"],"categories":[],"content":"","date":1621094229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621094229,"objectID":"12494cd1f3062f1cb382487ef8e410f7","permalink":"https://BinWangBFA.github.io/publication/icml21_copart/","publishdate":"2021-05-15T23:57:09+08:00","relpermalink":"/publication/icml21_copart/","section":"publication","summary":"This paper proposes an unsupervised Co-part segmentation approach, which leverages shape correlation information between different frames in the video to achieve semantic part segmentation. We have designed a novel network structure which achieves self-supervision through a dual procedure of part-assembly to form a closed loop with part-segmentation. Additionally, we have developed several new loss functions that ensure consistent, compact and meaningful part segmentation and the intermediate transformations with clear explainable physical meaning.","tags":["Computer Vision","Segmentation"],"title":"Unsupervised Co-part Segmentation through Assembly","type":"publication"},{"authors":["HONGDA JIANG —— CFCS, Peking University","MARC CHRISTIE —— University Rennes, Inria, CNRS, IRISA","XI WANG —— University Rennes, Inria, CNRS, IRISA","LIBIN LIU —— CFCS, Peking University","BIN WANG* —— Beijing Institute for General Artificial Intelligence","BAOQUAN CHEN* —— CFCS, Peking University","(* corresponding author)"],"categories":[],"content":"Demo   Method  Our proposed framework for learning camera together with keyframe constraints composed of a camera behavior extractor (Gating LSTM), which extracts camera behaviors from reference clips, and a camera motion generator, which generates camera trajectories that both meet the camera behaviors and required keyframe constraints, speed and directions.    By learning the mapping from a style code and several camera frames (5 in this paper) towards the starting hidden state of our autoregressive generator, we provide an additional degree of control for the users through the specification of camera velocities.    The proposed system enables designers to specify keyframes with initial velocities. In this example, the same keyframe positions and camera style code is used. As displayed, the resulting trajectories are guided by the different velocity directions defined at the starting keyframe (time 001) and the mid keyframe (time 090) respectively. This is achieved by updating the LSTM hidden state through a dedicated network which maps velocities with hidden states.    User-specified keyframes are placed at increasingly larger distances from the trajectory of a given style. As displayed, our system adapts well to the keyframes. We re-extract the style codes from the generated trajectories (shown with crosses in the PCA representation on the right part of the figure). As displayed our system moves from the given style to adapt to the keyframe constraints.    Our camera trajectory editing interface. The scene view (A) displays the animation. In the timeline (B) the user can add, drag, and delete keyframes (inverted triangles), as well as drag the process of animation. The keyframe editing (C) allows the user to select two target characters, shot view and Toric camera pose at the keyframe. The trajectories selection (D) provides generated camera trajectories with different behaviors for the user to choose. The button on the bottom is used to preview a result (Play), generate trajectories (Generate) and save results (Save).    Experiment with same keyframes and different behaviors: different colors represent different camera behaviors. Frames with red camera icon at the corner refer to keyframe constraints. We observe that constraints are well enforced in the 3D content and in the rendered snapshots.    Experiment with different keyframes and same behavior: different colors represent different keyframes fed to the system with the same style code. All three sequences belongs to a same style but their trajectories adjust well in response to the required keyframes (frames with different colors of camera icons at their corner).    This figure displays a result designed by an animation artist using only 10 keyframes for a 24 seconds sequence of a zombie fighting scene. We show the keyframes and camera trajectory simultaneously with the rendered animation snapshots.    In this hockey game scenario, our method is able to generate dynamic and qualitative camera motions using only 10 keyframes. Rendered animation snapshots and the overview trajectory are displayed.   Bibtex TBD\nAcknowledgement This work was supported in part by the National Key R\u0026amp;D Program of China (2018YFB1403900, 2019YFF0302902). We also thank Anthony Mirabile and Yulong Zhang for the various support and helpful discussions throughout this project, as well as Yu Xiong for his help processing the MovieNet dataset.\n","date":1620904308,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620904308,"objectID":"75e32bee830c400d28b8c7c188e204f3","permalink":"https://BinWangBFA.github.io/publication/sig21_camerakeyframe/","publishdate":"2021-05-13T19:11:48+08:00","relpermalink":"/publication/sig21_camerakeyframe/","section":"publication","summary":"We present a tool that enables artists to synthesize camera motions following a learned camera behavior while enforcing user-designed keyframes as constraints along the sequence.","tags":["Camera control","Animation"],"title":"Camera Keyframing with Style and Control","type":"publication"},{"authors":["BIN WANG^ —— AICFVE, Beijing Film Academy","YUANMIN DENG^ —— Shandong University \u0026 AICFVE, Beijing Film Academy","PAUL KRY —— McGill University","URI ASCHER —— University of British Columbia","HUI HUANG —— Shenzhen University","BAOQUAN CHEN* —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","(^ equal contribution, * corresponding author)"],"categories":[],"content":"","date":1600185429,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600185429,"objectID":"03007ac3a708b4a70571ed43efdcb6b2","permalink":"https://BinWangBFA.github.io/publication/pg20_deepm/","publishdate":"2020-09-15T23:57:09+08:00","relpermalink":"/publication/pg20_deepm/","section":"publication","summary":"This paper presents a new method for estimating nonlinear constitutive models from trajectories of surface data. The key insight is to have a parametric material correction model learn the error of the elastic and damping properties of a nominal material. ","tags":["Physical simulation","Physics"],"title":"Learning Elastic Constitutive Material and Damping Models","type":"publication"},{"authors":["XINGYU NI —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","BO ZHU —— Dartmouth College","BIN WANG* —— AICFVE, Beijing Film Academy","BAOQUAN CHEN* —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","(* corresponding author)"],"categories":[],"content":"","date":1589558229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589558229,"objectID":"a8a2b8e75a8849949756af182497ee62","permalink":"https://BinWangBFA.github.io/publication/sig20_ferrofluid/","publishdate":"2020-05-15T23:57:09+08:00","relpermalink":"/publication/sig20_ferrofluid/","section":"publication","summary":"This paper presents a versatile, numerical approach to simulating various magnetic phenomena using a level-set method, which contains a novel two-way coupling mechanism between a magnetic field and magnetizable objects.","tags":["Physical simulation","Physics"],"title":"A Level-Set Method for Magnetic Substance Simulation","type":"publication"},{"authors":["HONGDA JIANG^ —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","BIN WANG^ —— AICFVE, Beijing Film Academy","XI WANG —— University Rennes, Inria, CNRS, IRISA \u0026 AICFVE, Beijing Film Academy","MARC CHRISTIE —— University Rennes, Inria, CNRS, IRISA \u0026 AICFVE, Beijing Film Academy","BAOQUAN CHEN* —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","(^ equal contribution, * corresponding author)"],"categories":[],"content":"Demo   Method  Our proposed framework for learning camera behaviors composed of a Cinematic Feature Estimator which extracts high-level features from movie clips, a Gating network which estimates the type of camera behavior from the high-level features, and a Prediction network which applies the estimated behavior on a 3D animation.    To estimate cinematic features from film clips, each frame should pass through the following steps: (i) extracting 2D skeletons with LCR-Net, (ii) pose association, filling missing joints and smoothing, and (iii) estimating features through a neural network.    Learning stage of the Cinematic Feature Estimator. Input data is gathered over on a sliding window of $t_c=8$ frames to increase robustness during the learning and testing phases. The output data gathers cinematic features such as the camera pose estimation in Toric space and character features.    Structure of our Mixture of Experts (MoE) training network. The network takes as input the result of the Cinematic Feature Estimator applied on reference clips and the 3D animation. It outputs a sequence of camera parameters for each frame of the animation that can be used to render the animation.    Side track with different main character. In side track mode, the camera will always look from one side of the main character no matter his/her facing orientation. Green arrow indicates the main character in each sequence.    Relative vs direct track with same main character. Relative track (top) puts more emphasis on the male character; while the feeling conveyed by direct track (bottom) is more objective. Green arrow indicates the main character in each sequence.    Snapshots taken from two distinct camera motions computed on the zombie sequence using two distinct sequences of input. The green arrow represents the main character. As viewed in the snapshots, there are changes in the main character throughout the sequence. This enables the designer, through selected clips, to emphasize one character over another at different moments.   Bibtex TBD\nAcknowledgement This work was supported in part by the National Key R\u0026amp;D Program of China (2018YFB1403900, 2019YFF0302902). We also thank Anthony Mirabile and Ludovic Burg from University Rennes, Inria, CNRS, IRISA and Di Zhang from AICFVE, Beijing Film Academy for their help in animation generation and rendering.\n","date":1589368308,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589368308,"objectID":"85dd3203c205897c04e7582c724b90f7","permalink":"https://BinWangBFA.github.io/publication/sig20_camerabehavior/","publishdate":"2020-05-13T19:11:48+08:00","relpermalink":"/publication/sig20_camerabehavior/","section":"publication","summary":"In this paper, we propose an example-driven camera controller which can extract camera behaviors from an example film clip and re-apply the extracted behaviors to a 3D animation, through learning from a collection of camera motions.","tags":["Camera control","Animation"],"title":"Example-driven Virtual Cinematography by Learning Camera Behaviors","type":"publication"},{"authors":["Bin Wang —— Shenzhen VisuCA Key Lab / SIAT, National University of Singapore","Longhua Wu —— Shenzhen VisuCA Key Lab / SIAT","KangKang Yin —— National University of Singapore","Uri Ascher —— University of British Columbia","Libin Liu —— University of British Columbia","Hui Huang —— Shenzhen VisuCA Key Lab / SIAT"],"categories":[],"content":"Acknowledgement We thank the anonymous reviewers for their constructive comments. We are grateful to the authors of [Chen et al. 2014] for sharing their data and fabricated models for our validation and comparison experiments. We also thank Jiacheng Ren, Jiangtao Shen, Keng Hua Sing, Francois Faure and Matthieu Nesme for their help and thoughtful discussions at the various stages of developing this project. This work is supported in part by Singapore Ministry of Education Academic Research Fund Tier 2 (MOE2011-T2-2-152); Microsoft Research Asia Collaborative Research Program (FY14-RES-OPP-002); NSFC (61402459, 61379090, 61331018); National 973 Program (2014CB360503); Shenzhen Innovation Program (CXB201104220029A, JCYJ20140901003939034, JCYJ20130401170306810); NSERC Discovery Grant (84306).\n","date":1431706269,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1431706269,"objectID":"0337f3a674d41720d4af2aa8d504080e","permalink":"https://BinWangBFA.github.io/publication/sig15_deformationcapture/","publishdate":"2020-05-16T00:11:09+08:00","relpermalink":"/publication/sig15_deformationcapture/","section":"publication","summary":"We present a data-driven method that can capture deformation of generic soft objects in high fidelity with low-cost depth sensors; and estimate plausible deformation parameters from these pure kinematic motion trajectories, without requiring any force-displacement measurements as is common in traditional methods. Using the learned deformation models, new motion and deformation can be synthesized at interactive rates to respond to dynamic perturbations or satisfy user-specified constraints. ","tags":[],"title":"Deformation Capture and Modeling of Soft Objects","type":"publication"},{"authors":["Libin Liu —— Tsing Hua University","Kangkang Yin —— National University of Singapore","Bin Wang —— National University of Singapore","Baining Guo —— Microsoft Research Asia"],"categories":[],"content":"","date":1368635845,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1368635845,"objectID":"fa3630e83bf05444713930569158e1b7","permalink":"https://BinWangBFA.github.io/publication/sig13_softcharacter/","publishdate":"2020-05-16T00:37:25+08:00","relpermalink":"/publication/sig13_softcharacter/","section":"publication","summary":"We have presented a two-way simulation and control framework for soft characters with inherent skeletons. We propose a novel pose-based plasticity model that extends the corotated linear elasticity model to achieve large skin deformation around joints. We further reconstruct controls from reference trajectories captured from human subjects by augmenting a sampling-based algorithm.","tags":[],"title":"Simulation and Control of Skeleton-driven Soft Body Characters","type":"publication"},{"authors":["Bin Wang -- Beihang University, University of British Columbia","François Faure -- University of Grenoble, INRIA, LJK-CNRS, University of British Columbia","Dinesh K. Pai -- University of British Columbia"],"categories":[],"content":"","date":1337098284,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1337098284,"objectID":"723e0862dae4c66c8c4250d0e27fcdb8","permalink":"https://BinWangBFA.github.io/publication/sig12_adaptivecontact/","publishdate":"2020-05-16T00:11:24+08:00","relpermalink":"/publication/sig12_adaptivecontact/","section":"publication","summary":"The accuracy of intersection volume is important for plausible collision response. In this paper we have presented the first imagebased collision detection method that provides the controllability of intersection volume without explicit geometrical computation, and demonstrated its relevance for precise contact modeling. Its computation combines rasterization at moderate resolution with adaptive ray casting, which allows more precise contact modeling where needed and a reduced memory footprint","tags":[],"title":"Adaptive Image-based Intersection Volume","type":"publication"}]