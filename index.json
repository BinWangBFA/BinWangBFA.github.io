[{"authors":["admin"],"categories":null,"content":"Bin Wang is currently a full-time researcher of Advanced Innovation Center for Future Visual Entertainment in Beijing Film Academy. She earned her Ph.D in November 2012 from Beihang University, advised by Prof. Shuling Dai. From Aug. 2010 to Aug. 2012, she experienced two-year\u0026rsquo;s wonderful life in the Sensorimotor System Lab at University of British Columbia, Canada, working with Prof. Dinesh K. Pai and Prof. François Faure, from University Joseph Fourier, Grenoble, France. She has been a Research Fellow from April 2013 to April 2014 in National University of Singapore, supervised by Prof. Kangkang Yin. From 2014 to 2016, she did her postdoctoral researcher at Visual Computing Research Center, leading by Prof. Hui Huang.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://BinWangBFA.github.io/author/bin-wang-%E7%8E%8B-%E6%BB%A8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bin-wang-%E7%8E%8B-%E6%BB%A8/","section":"authors","summary":"Bin Wang is currently a full-time researcher of Advanced Innovation Center for Future Visual Entertainment in Beijing Film Academy. She earned her Ph.D in November 2012 from Beihang University, advised","tags":null,"title":"Bin Wang (王 滨)","type":"authors"},{"authors":["XINGYU NI —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","BO ZHU —— Dartmouth College","BIN WANG* —— AICFVE, Beijing Film Academy","BAOQUAN CHEN* —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","(* denotes equal contribution)"],"categories":[],"content":"","date":1589558229,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589558229,"objectID":"a8a2b8e75a8849949756af182497ee62","permalink":"https://BinWangBFA.github.io/publication/sig20_ferrofluid/","publishdate":"2020-05-15T23:57:09+08:00","relpermalink":"/publication/sig20_ferrofluid/","section":"publication","summary":"This paper presents a versatile, numerical approach to simulating various magnetic phenomena using a level-set method, which contains a novel two-way coupling mechanism between a magnetic field and magnetizable objects.","tags":["Physical simulation","Physics"],"title":"A Level-Set Method for Magnetic Substance Simulation","type":"publication"},{"authors":["HONGDA JIANG* —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","BIN WANG* —— AICFVE, Beijing Film Academy","XI WANG —— University Rennes, Inria, CNRS, IRISA \u0026 AICFVE, Beijing Film Academy","MARC CHRISTIE —— University Rennes, Inria, CNRS, IRISA \u0026 AICFVE, Beijing Film Academy","BAOQUAN CHEN —— CFCS, Peking University \u0026 AICFVE, Beijing Film Academy","(* denotes equal contribution)"],"categories":[],"content":"Demo   Method  Our proposed framework for learning camera behaviors composed of a Cinematic Feature Estimator which extracts high-level features from movie clips, a Gating network which estimates the type of camera behavior from the high-level features, and a Prediction network which applies the estimated behavior on a 3D animation.    To estimate cinematic features from film clips, each frame should pass through the following steps: (i) extracting 2D skeletons with LCR-Net, (ii) pose association, filling missing joints and smoothing, and (iii) estimating features through a neural network.    Learning stage of the Cinematic Feature Estimator. Input data is gathered over on a sliding window of $t_c=8$ frames to increase robustness during the learning and testing phases. The output data gathers cinematic features such as the camera pose estimation in Toric space and character features.    Structure of our Mixture of Experts (MoE) training network. The network takes as input the result of the Cinematic Feature Estimator applied on reference clips and the 3D animation. It outputs a sequence of camera parameters for each frame of the animation that can be used to render the animation.    Side track with different main character. In side track mode, the camera will always look from one side of the main character no matter his/her facing orientation. Green arrow indicates the main character in each sequence.    Relative vs direct track with same main character. Relative track (top) puts more emphasis on the male character; while the feeling conveyed by direct track (bottom) is more objective. Green arrow indicates the main character in each sequence.    Snapshots taken from two distinct camera motions computed on the zombie sequence using two distinct sequences of input. The green arrow represents the main character. As viewed in the snapshots, there are changes in the main character throughout the sequence. This enables the designer, through selected clips, to emphasize one character over another at different moments.   Bibtex TBD\nAcknowledgement This work was supported in part by the National Key R\u0026amp;D Program of China (2018YFB1403900, 2019YFF0302902). We also thank Anthony Mirabile and Ludovic Burg from University Rennes, Inria, CNRS, IRISA and Di Zhang from AICFVE, Beijing Film Academy for their help in animation generation and rendering.\n","date":1589368308,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589368308,"objectID":"85dd3203c205897c04e7582c724b90f7","permalink":"https://BinWangBFA.github.io/publication/sig20_camerabehavior/","publishdate":"2020-05-13T19:11:48+08:00","relpermalink":"/publication/sig20_camerabehavior/","section":"publication","summary":"In this paper, we propose an example-driven camera controller which can extract camera behaviors from an example film clip and re-apply the extracted behaviors to a 3D animation, through learning from a collection of camera motions.","tags":["Camera control","Animation"],"title":"Example-driven Virtual Cinematography by Learning Camera Behaviors","type":"publication"},{"authors":["Bin Wang —— Shenzhen VisuCA Key Lab / SIAT, National University of Singapore","Longhua Wu —— Shenzhen VisuCA Key Lab / SIAT","KangKang Yin —— National University of Singapore","Uri Ascher —— University of British Columbia","Libin Liu —— University of British Columbia","Hui Huang —— Shenzhen VisuCA Key Lab / SIAT"],"categories":[],"content":"Acknowledgement We thank the anonymous reviewers for their constructive comments. We are grateful to the authors of [Chen et al. 2014] for sharing their data and fabricated models for our validation and comparison experiments. We also thank Jiacheng Ren, Jiangtao Shen, Keng Hua Sing, Francois Faure and Matthieu Nesme for their help and thoughtful discussions at the various stages of developing this project. This work is supported in part by Singapore Ministry of Education Academic Research Fund Tier 2 (MOE2011-T2-2-152); Microsoft Research Asia Collaborative Research Program (FY14-RES-OPP-002); NSFC (61402459, 61379090, 61331018); National 973 Program (2014CB360503); Shenzhen Innovation Program (CXB201104220029A, JCYJ20140901003939034, JCYJ20130401170306810); NSERC Discovery Grant (84306).\n","date":1431706269,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1431706269,"objectID":"0337f3a674d41720d4af2aa8d504080e","permalink":"https://BinWangBFA.github.io/publication/sig15_deformationcapture/","publishdate":"2020-05-16T00:11:09+08:00","relpermalink":"/publication/sig15_deformationcapture/","section":"publication","summary":"We present a data-driven method that can capture deformation of generic soft objects in high fidelity with low-cost depth sensors; and estimate plausible deformation parameters from these pure kinematic motion trajectories, without requiring any force-displacement measurements as is common in traditional methods. Using the learned deformation models, new motion and deformation can be synthesized at interactive rates to respond to dynamic perturbations or satisfy user-specified constraints. ","tags":[],"title":"Deformation Capture and Modeling of Soft Objects","type":"publication"},{"authors":["Libin Liu —— Tsing Hua University","Kangkang Yin —— National University of Singapore","Bin Wang —— National University of Singapore","Baining Guo —— Microsoft Research Asia"],"categories":[],"content":"","date":1368635845,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1368635845,"objectID":"fa3630e83bf05444713930569158e1b7","permalink":"https://BinWangBFA.github.io/publication/sig13_softcharacter/","publishdate":"2020-05-16T00:37:25+08:00","relpermalink":"/publication/sig13_softcharacter/","section":"publication","summary":"We have presented a two-way simulation and control framework for soft characters with inherent skeletons. We propose a novel pose-based plasticity model that extends the corotated linear elasticity model to achieve large skin deformation around joints. We further reconstruct controls from reference trajectories captured from human subjects by augmenting a sampling-based algorithm.","tags":[],"title":"Simulation and Control of Skeleton-driven Soft Body Characters","type":"publication"},{"authors":["Bin Wang -- Beihang University, University of British Columbia","François Faure -- University of Grenoble, INRIA, LJK-CNRS, University of British Columbia","Dinesh K. Pai -- University of British Columbia"],"categories":[],"content":"","date":1337098284,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1337098284,"objectID":"723e0862dae4c66c8c4250d0e27fcdb8","permalink":"https://BinWangBFA.github.io/publication/sig12_adaptivecontact/","publishdate":"2020-05-16T00:11:24+08:00","relpermalink":"/publication/sig12_adaptivecontact/","section":"publication","summary":"The accuracy of intersection volume is important for plausible collision response. In this paper we have presented the first imagebased collision detection method that provides the controllability of intersection volume without explicit geometrical computation, and demonstrated its relevance for precise contact modeling. Its computation combines rasterization at moderate resolution with adaptive ray casting, which allows more precise contact modeling where needed and a reduced memory footprint","tags":[],"title":"Adaptive Image-based Intersection Volume","type":"publication"}]